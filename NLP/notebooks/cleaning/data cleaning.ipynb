{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing the text**\n",
    "\n",
    "- Lowercase Conversion: Convert the text to lower case.\n",
    "- Remove Stopwords: Remove common words like \"and\" , \"is\" , \"the\" using stop word list\n",
    "- tokenization: Split the text into individual words or phrases.\n",
    "- Remove Punctuation and Special Characters: Clean the text by removing non-aphanumeric characters.\n",
    "- Lemmatization/Stemming: Reduce words to their base or root from to avoid duplication eg \"running\" -> \"run\"\n",
    "\n",
    "\n",
    "example : \"this course provides an in-depth introduction to python programming, focusing on data analysis, machine learning and automation.\"\n",
    "\n",
    "after processing: [\"course\", \"provides\", \"introduction\", \"python\", \"programming\", \"focusing\", \"data\", \"analysis\", \"machine\", \"learning\", \"automation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lowercasing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This provides an in-depth introduction to Python programming, focusing on data analysis, machine learning, and automation.\"\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Stopwords**\n",
    "\n",
    "There are 3 ways to remove Stopwords\n",
    "\n",
    "- NLTK\n",
    "- spaCy\n",
    "- Textcleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "stop = set(stopwords.words('english'))\n",
    "filtered = [w for w in tokens if not w.lower() in stop]\n",
    "print(tokens)\n",
    "print(filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy is not able to insall in this jupiter notebook but got it in file\n",
    "\n",
    "\n",
    "link - E:\\C125\\python\\data cleaning\\spacy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textcleaner import clean\n",
    "\n",
    "text = \"This is a sample text with some HTML tags like <br> and special characters like &amp; and some numbers 123.\"\n",
    "cleaned_text = clean(text)\n",
    "print(cleaned_text)\n",
    "\n",
    "text = \"This is a sample text with some \\t tabs and \\n new lines.\"\n",
    "cleaned_text = clean(text, tabs=True, new_lines=True)\n",
    "print(cleaned_text)\n",
    "\n",
    "text = \"This is a sample text with some extra spaces.   \"\n",
    "cleaned_text = clean(text, extra_spaces=True)\n",
    "print(cleaned_text)\n",
    "\n",
    "text = \"This is a sample text with some punctuation!?\"\n",
    "cleaned_text = clean(text, punctuation=True)\n",
    "print(cleaned_text)\n",
    "\n",
    "text = \"This is a sample text with some numbers 12345.\"\n",
    "cleaned_text = clean(text, numbers=True)\n",
    "print(cleaned_text)\n",
    "\n",
    "\n",
    "text = \"This is a sample text with some accents like éàçüö.\"\n",
    "cleaned_text = clean(text, accents=True)\n",
    "print(cleaned_text)\n",
    "\n",
    "text = \"This is a sample text with some brackets like []{}()<>.\"\n",
    "cleaned_text = clean(text, brackets=True)\n",
    "print(cleaned_text)\n",
    "\n",
    "text = \"This is a sample text with some quotes like ''\\\"\\\".\"\n",
    "cleaned_text = clean(text, quotes=True)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With re module Data Cleaning** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic data cleaning \n",
    "\n",
    "import re\n",
    "\n",
    "text = \" This course covers advanced topics in <b>Deep Learning</b> and Neural Networks.   Extra spaces\"\n",
    "\n",
    "text = text.lower()\n",
    "text = re.sub(r\"<.*?>\",\"\",text)  #removing HTML tags\n",
    "text = re.sub(r\"&amp\",\"&\",text)  #removing HTML entities\n",
    "text = re.sub(r\"[^\\x00-\\x7F]+\",\"\",text)  #removing non-ASCII charcaters\n",
    "text = text.replace('\\n','').replace('\\t', ' ') # remoing next line and tabs\n",
    "text = re.sub(r\"\\s+\",\" \",text).split()  #remvoing extra spaces\n",
    "\n",
    "print(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods to tokenize:\n",
    "- nltk : \n",
    "    - word_tokenize() : it uesee the treebandk word tokenizer, which handles contractions(eg, \"don't\" becomes \"do\" and \"n't\") and punctuation reasonably well.\n",
    "    - sent_tokenize(): splites text into sentences.\n",
    "    - TreebankWordTokenizer() : this is the underlying tokenizer used by word_tokenizer(). you can use it directly for more control.\n",
    "\n",
    "    Other Tokenizer : NLTK also provides other tokenizer like WHitespaceTokenizer , PunkWordTokenizer ,and regular expression-based tokenizers for more specialized use cases.\n",
    "\n",
    "- spaCy : it's tokenization is rule_based and highly optimized for speed and accuracy. it's integrated into its processing pipeline.\n",
    "    \n",
    "\n",
    "- Other Libraries/Methods:\n",
    "    - str.split(): the simplest form of tokenization\n",
    "\n",
    "    - Regular Expression(re.findall()): you can use regular expression for more complex tokenization patterns.\n",
    "\n",
    "    - Hugging face Tokenizers : The tokenizers libraru for Hugging Face is widely used for transformer-based models. It offers various Tokenization Algorithms like WordPiecs, Byte-Pair Encoding(BPE),and SentencePiece, Which are often used in deep learning models for NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_tokenizer()\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordtokenizer\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "text = \"this is an example sentence. It's Tokenized\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "tokens2 = TreebankWordtokenizer().tokenize(text)\n",
    "\n",
    "print(tokens)\n",
    "print(sentences)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy is in python file - data cleaning(spacy).py\n",
    "#str.split()\n",
    "text = \"this is a sample text\"\n",
    "tokens = text.split()\n",
    "print(tokens)\n",
    "\n",
    "#using regular explression\n",
    "\n",
    "import re\n",
    "\n",
    "text = \"This is an example-with-hyphens.\"\n",
    "tokens = re.findall(r\"[\\w]+-[\\w]+|[\\w]+|[\\W]\", text)  # Matches hyphenated words, words, or non-word characters\n",
    "print(tokens)\n",
    "# Output: ['This', 'is', 'an', 'example-with-hyphens', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Punctuation and special characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using re\n",
    "import re\n",
    "\n",
    "text = \"this is a sample text whith 4$%(#)\"\n",
    "text = re.sub(r\"[^\\w+]\",' ',text)\n",
    "print(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmiatization or Stemming or a word**\n",
    "\n",
    "- Stemming : A simpler approach that involves chopping off prefixes or suffixes of words based on heuristics. it often results in non-words (eg. running , becommes, runn)\n",
    "- Lemmatization: A more sophisticated approach that uses a vocabolary and morphological analysis to find the base or dictionary form of word(lemma). it produces actual words (eg. running becomes run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**done in file - data cleaning(Stemming and lemmatization).py**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
