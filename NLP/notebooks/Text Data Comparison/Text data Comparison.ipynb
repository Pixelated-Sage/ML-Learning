{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methods of Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cosine Similarity*\n",
    "- How It Works: Measures the cosine of the angle between two vectors. Focuses on the direction of the vectors, ignoring their magnitude.\n",
    "- When to Use: When the length of the text (magnitude of vectors) doesn't matter, but the pattern of words does.\n",
    "- Pros:\n",
    "    - Works well with TF-IDF vectors.\n",
    "    - Commonly used for text-based recommendations.\n",
    "- Cons:\n",
    "    - May not capture semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Euclidean Distance*\n",
    "- How It Works: Calculates the straight-line distance between two vectors.\n",
    "- When to Use: When the magnitude (size) of vectors is important.\n",
    "- Pros:\n",
    "    - Intuitive and simple to compute.\n",
    "- Cons:\n",
    "    -Sensitive to vector magnitude, so normalization is often required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Jaccard Similarity*\n",
    "- How It Works: Compares the intersection over the union of two sets (e.g., word sets or n-grams).\n",
    "- When to Use: When comparing the overlap of terms is more important than their frequency or context.\n",
    "- Pros:\n",
    "    - Useful for binary data or token-based comparisons.\n",
    "- Cons:\n",
    "    - Ignores term frequency and context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pearson Correlation*\n",
    "- How It Works: Measures the linear correlation between two vectors.\n",
    "- When to Use: When you're interested in the degree to which the two vectors change together.\n",
    "- Pros:\n",
    "    - Captures linear relationships well.\n",
    "- Cons:\n",
    "    - Not commonly used for text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Soft Cosine Similarity*\n",
    "- How It Works: Extends cosine similarity by accounting for the similarity between words (e.g., synonyms).\n",
    "- When to Use: When you want to include semantic similarity (e.g., \"AI\" and \"Artificial Intelligence\").\n",
    "- Pros:\n",
    "    - Incorporates word embeddings for richer comparisons.\n",
    "- Cons:\n",
    "    - More computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pre-trained Embeddings with Similarity Metrics*\n",
    "- How It Works: Uses pre-trained word embeddings (e.g., Word2Vec, GloVe, or BERT) to represent text and then calculates similarity (e.g., cosine similarity) on the embeddings.\n",
    "- When to Use: When you want to capture semantic meaning and contextual relationships between words.\n",
    "- Pros:\n",
    "    - Captures semantic meaning.\n",
    "    - State-of-the-art performance for many tasks.\n",
    "- Cons:\n",
    "    - Requires more computational resources.\n",
    "    - May need fine-tuning for specific datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from scipy.spatial.distance import jaccard\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.matutils import softcossim\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Example dataset\n",
    "Samples = [\n",
    "    \"Advanced Machine Learning and Deep Learning, with focus on Neural Networks and AI.\",\n",
    "    \"Introduction to Programming in Python and Basics of AI.\",\n",
    "    \"Data Science and Machine Learning using Python and AI techniques.\",\n",
    "]\n",
    "\n",
    "# Preprocessing: Lowercase and simple tokenization\n",
    "cleaned_samples = [desc.lower().replace(\",\", \"\").replace(\".\", \"\") for desc in Samples]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(cleaned_samples)\n",
    "\n",
    "# Method 1: Cosine Similarity\n",
    "print(\"Cosine Similarity:\")\n",
    "cos_sim = cosine_similarity(tfidf_matrix)\n",
    "print(cos_sim)\n",
    "\n",
    "# Method 2: Euclidean Distance\n",
    "print(\"\\nEuclidean Distance:\")\n",
    "euclidean_dist = euclidean_distances(tfidf_matrix)\n",
    "print(euclidean_dist)\n",
    "\n",
    "# Method 3: Jaccard Similarity\n",
    "def jaccard_similarity(str1, str2):\n",
    "    set1, set2 = set(str1.split()), set(str2.split())\n",
    "    return len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "print(\"\\nJaccard Similarity:\")\n",
    "jaccard_sim = np.zeros((len(cleaned_samples), len(cleaned_samples)))\n",
    "for i in range(len(cleaned_samples)):\n",
    "    for j in range(len(cleaned_samples)):\n",
    "        jaccard_sim[i, j] = jaccard_similarity(cleaned_samples[i], cleaned_samples[j])\n",
    "print(jaccard_sim)\n",
    "\n",
    "# Method 4: Soft Cosine Similarity\n",
    "print(\"\\nSoft Cosine Similarity:\")\n",
    "# Load pre-trained word vectors (replace with actual file path to word embeddings like GloVe or Word2Vec)\n",
    "# word_vectors = KeyedVectors.load_word2vec_format(\"path/to/word2vec/file\", binary=True)\n",
    "# For demonstration, we create a dummy word embedding dictionary\n",
    "dummy_word_vectors = {\n",
    "    \"advanced\": np.random.rand(100),\n",
    "    \"machine\": np.random.rand(100),\n",
    "    \"learning\": np.random.rand(100),\n",
    "    \"python\": np.random.rand(100),\n",
    "    \"ai\": np.random.rand(100),\n",
    "    \"data\": np.random.rand(100),\n",
    "    \"science\": np.random.rand(100),\n",
    "}\n",
    "\n",
    "# Create a Gensim dictionary and similarity matrix\n",
    "dictionary = Dictionary([desc.split() for desc in cleaned_samples])\n",
    "similarity_matrix = np.zeros((len(dictionary), len(dictionary)))\n",
    "\n",
    "# Fill the similarity matrix using dummy vectors\n",
    "for i, word1 in enumerate(dictionary.token2id.keys()):\n",
    "    for j, word2 in enumerate(dictionary.token2id.keys()):\n",
    "        vec1, vec2 = dummy_word_vectors.get(word1, np.zeros(100)), dummy_word_vectors.get(word2, np.zeros(100))\n",
    "        similarity_matrix[i, j] = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-10)\n",
    "\n",
    "# Soft Cosine Similarity\n",
    "soft_cos_sim = np.zeros((len(cleaned_samples), len(cleaned_samples)))\n",
    "for i, desc1 in enumerate(cleaned_samples):\n",
    "    bow1 = dictionary.doc2bow(desc1.split())\n",
    "    for j, desc2 in enumerate(cleaned_samples):\n",
    "        bow2 = dictionary.doc2bow(desc2.split())\n",
    "        soft_cos_sim[i, j] = softcossim(bow1, bow2, similarity_matrix)\n",
    "\n",
    "print(soft_cos_sim)\n",
    "\n",
    "# Method 5: Pre-trained Embeddings with Cosine Similarity\n",
    "print(\"\\nPre-trained Embeddings with Cosine Similarity:\")\n",
    "# Generate dummy sentence embeddings by averaging word vectors\n",
    "sentence_embeddings = []\n",
    "for desc in cleaned_samples:\n",
    "    vectors = [dummy_word_vectors.get(word, np.zeros(100)) for word in desc.split()]\n",
    "    sentence_embeddings.append(np.mean(vectors, axis=0))\n",
    "\n",
    "# Compute cosine similarity for sentence embeddings\n",
    "pretrained_cos_sim = cosine_similarity(sentence_embeddings)\n",
    "print(pretrained_cos_sim)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
